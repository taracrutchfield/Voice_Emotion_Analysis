# Capstone 3 Project (WIP)

In this project I hope to create a model to determine the emotion in a persons voice. As of the last update, the audio I'm using comes from [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D) and [RAVDESS](https://smartlaboratory.org/ravdess/) which are two databases consiting of different phrases voiced by actors portaying a specific emotion. Overall the data includes total of 8877 voice clips with 14 different phrases and 8 seperate emotions including neutral, calm, happy, sad, anger, fear, disgust, and surprised.

Currently I'm working on extracting features from the audio clip, hoping to include qualities such as chroma features and mel-Frequency Cepstral Coefficients. Future plans inlcude using the final data set to train and test both convolutional and recurrent neural network models and potentially more depending on time. 
